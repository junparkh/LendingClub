{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    "\n",
    "In the a previous homework assignment, you used two different dynamic programming algorithms to solve a robot navigation problem by finding optimal paths to a goal in a simplified warehouse environment. Now you will use first visit Monte Carlo reinforcement learning to find optimal paths in the same environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration of the warehouse environment is illustrated in the figure below.\n",
    "\n",
    "<img src=\"GridWorldFactory.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **Grid World for Factory Navigation Example** </center>\n",
    "\n",
    "The goal is for the robot to deliver some material to position (state) 12, shown in blue. Since there is a goal state or **terminal state** this an **episodic task**. \n",
    "\n",
    "There are some barriers comprised of the states $\\{ 6, 7, 8 \\}$ and $\\{ 16, 17, 18 \\}$, shown with hash marks. In a real warehouse, these positions might be occupied by shelving or equipment. We do not want the robot to hit these barriers. Thus, we say that transitioning to these barrier states is **taboo**.\n",
    "\n",
    "As before, we do not want the robot to hit the edges of the grid world, which represent the outer walls of the warehouse. \n",
    "\n",
    "## Representation\n",
    "\n",
    "As with many such problems, the starting place is creating the **representation**. In the cell below encode your representation for the possible action-state transitions. From each state there are 4 possible actions:\n",
    "- up, u\n",
    "- down, d,\n",
    "- left, l\n",
    "- right, r\n",
    "\n",
    "There are a few special cases you need to consider:\n",
    "- Any action transitioning state off the grid or into a barrier should keep the state unchanged. \n",
    "- Any action in the goal state keeps the state unchanged. \n",
    "- Any transition within the taboo (barrier) states can keep the state unchanged. If you experiment, you will see that other encodings work as well since the value of a barrier states are always zero and there are no actions transitioning into these states. \n",
    "\n",
    "> **Hint:** It may help you create a pencil and paper sketch of the transitions, rewards, and probabilities or policy. This can help you to keep the bookkeeping correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## import numpy for latter\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neighbors = {0:{'u':0, 'd':5, 'l':0, 'r':1},\n",
    "          1:{'u':1, 'd':1, 'l':0, 'r':2},\n",
    "          2:{'u':2, 'd':2, 'l':1, 'r':3},\n",
    "          3:{'u':3, 'd':3, 'l':2, 'r':4},\n",
    "          4:{'u':4, 'd':9, 'l':3, 'r':4},\n",
    "          \n",
    "          5:{'u':0, 'd':10, 'l':5, 'r':5},\n",
    "          6:{'u':6, 'd':6, 'l':6, 'r':6},###barrier\n",
    "          7:{'u':7, 'd':7, 'l':7, 'r':7},###barrier\n",
    "          8:{'u':8, 'd':8, 'l':8, 'r':8},###barrier\n",
    "          9:{'u':4, 'd':14, 'l':9, 'r':9},\n",
    "          \n",
    "          10:{'u':5, 'd':15, 'l':10, 'r':11},\n",
    "          11:{'u':11, 'd':11, 'l':10, 'r':12},\n",
    "          12:{'u':12, 'd':12, 'l':12, 'r':12},#goal\n",
    "          13:{'u':13, 'd':13, 'l':12, 'r':14},\n",
    "          14:{'u':9, 'd':19, 'l':13, 'r':14},\n",
    "          \n",
    "          15:{'u':10, 'd':20, 'l':15, 'r':15},\n",
    "          16:{'u':16, 'd':16, 'l':16, 'r':16},###barrier\n",
    "          17:{'u':17, 'd':17, 'l':17, 'r':17},###barrier\n",
    "          18:{'u':18, 'd':18, 'l':18, 'r':18},###barrier\n",
    "          19:{'u':14, 'd':24, 'l':19, 'r':19},\n",
    "          \n",
    "          20:{'u':15, 'd':20, 'l':20, 'r':21},\n",
    "          21:{'u':21, 'd':21, 'l':20, 'r':22},\n",
    "          22:{'u':22, 'd':22, 'l':21, 'r':23},\n",
    "          23:{'u':23, 'd':23, 'l':22, 'r':24},\n",
    "          24:{'u':19, 'd':24, 'l':23, 'r':24}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to define the initial transition probabilities for the Markov process. Set the probabilities for each transition as a **uniform distribution** leading to random action by the robot. \n",
    "\n",
    "> **Note:** As these are just starting values, the exact values of the transition probabilities are not actually all that important in terms of solving the MC RL problem. Also, notice that it does not matter how the taboo state transitions are encoded. The point of the MC RL algorithm is to learn a model including the transition policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy = {0:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          1:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          2:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          3:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          4:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          \n",
    "          5:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          6:{'u':0, 'd':0, 'l':0, 'r':0},###barrier\n",
    "          7:{'u':0, 'd':0, 'l':0, 'r':0},###barrier\n",
    "          8:{'u':0, 'd':0, 'l':0, 'r':0},###barrier\n",
    "          9:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          \n",
    "          10:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          11:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          12:{'u':0, 'd':0, 'l':0, 'r':0},#goal\n",
    "          13:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          14:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          \n",
    "          15:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          16:{'u':0, 'd':0, 'l':0, 'r':0},###barrier\n",
    "          17:{'u':0, 'd':0, 'l':0, 'r':0},###barrier\n",
    "          18:{'u':0, 'd':0, 'l':0, 'r':0},###barrier\n",
    "          19:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          \n",
    "          20:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          21:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          22:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          23:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "          24:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robot receives the following rewards:\n",
    "- 10 for entering position 0. \n",
    "- -1 for attempting to leave the grid. In other words, we penalize the robot for hitting the edges of the grid.  \n",
    "- -0.1 for all other state transitions, which is the cost for the robot to move from one state to another. If we did not have this penalty, the robot could follow any random plan to the goal which did not hit the edges. \n",
    "\n",
    "This **reward structure is unknown to the MC RL agent**. The agent must **learn** the rewards by sampling the environment. \n",
    "\n",
    "In the code cell below encode your representation of this reward structure you will use in your simulated environment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reward = {0:{'u':-1, 'd':-0.1, 'l':-1, 'r':-0.1},\n",
    "          1:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          2:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          3:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          4:{'u':-1, 'd':-0.1, 'l':-1, 'r':-0.1},\n",
    "          \n",
    "          5:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "          6:{'u':0, 'd':0, 'l':0, 'r':0},###barrier\n",
    "          7:{'u':0, 'd':0, 'l':0, 'r':0},###barrier\n",
    "          8:{'u':0, 'd':0, 'l':0, 'r':0},###barrier\n",
    "          9:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "          \n",
    "          10:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-0.1},\n",
    "          11:{'u':-1, 'd':-1, 'l':-0.1, 'r':10},\n",
    "          12:{'u':0, 'd':0, 'l':0, 'r':0},#goal\n",
    "          13:{'u':-1, 'd':-1, 'l':10, 'r':-0.1},\n",
    "          14:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1},\n",
    "          \n",
    "          15:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "          16:{'u':0, 'd':0, 'l':0, 'r':0},###barrier\n",
    "          17:{'u':0, 'd':0, 'l':0, 'r':0},###barrier\n",
    "          18:{'u':0, 'd':0, 'l':0, 'r':0},###barrier\n",
    "          19:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "          \n",
    "          20:{'u':-0.1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          21:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          22:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          23:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          24:{'u':-0.1, 'd':-1, 'l':-0.1, 'r':-1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find it useful to create a list of taboo states, which you can encode in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 16, 17, 18, 12]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taboo = [6,7,8,16,17,18]\n",
    "taboo+[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "With your representations defined, you can now create and test functions to perform MC RL **policy evaluation**. You will need these functions for your policy improvement code. \n",
    "\n",
    "As a first step you will need a function to generate episodes given the policy in the warehouse environment. You are welcome to start with the `MC_generate_episode` function from the MC RL notebook. However, keep in mind that you must modify this code to correctly treat the taboo states of the barrier. Specifically, taboo states will not be visited on the random walk. \n",
    "\n",
    "Set a seed of `4567` and execute your code to test it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def MC_generate_episode(policy, neighbors, terminal):\n",
    "    ## List of states which might be visited in episode\n",
    "    n_states = len(policy)\n",
    "#    visited_state = [0] * n_states\n",
    "    states = list(neighbors.keys())\n",
    "    \n",
    "    ## Random starting state for this episode, but can't be the terminal state\n",
    "    current_state = nr.choice(states, size = 1)[0]\n",
    "    while(current_state in taboo+[terminal]): # Keep trying to not use terminal state to start\n",
    "        current_state = nr.choice(states, size = 1)[0]\n",
    "            \n",
    "    ## Take a random walk trough the states until we get to the terminal state\n",
    "    ## We do some bookkeeping to ensure we only visit states once.\n",
    "    visited = [] # List of states visited on random walk\n",
    "    while(current_state != terminal): # Stop when at terminal state\n",
    "        ## Probability of state transition given policy\n",
    "        probs = list(policy[current_state].values())\n",
    "        ## Find next state to transition to\n",
    "        next_state = nr.choice(list(neighbors[current_state].values()), size = 1, p = probs)[0]\n",
    "        visited.append(next_state)\n",
    "        current_state = next_state  \n",
    "    return(visited)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "False\n",
      "20\n",
      "False\n",
      "20\n",
      "False\n",
      "21\n",
      "False\n",
      "21\n",
      "False\n",
      "21\n",
      "False\n",
      "21\n",
      "False\n",
      "21\n",
      "False\n",
      "22\n",
      "False\n",
      "23\n",
      "False\n",
      "23\n",
      "False\n",
      "24\n",
      "False\n",
      "24\n",
      "False\n",
      "23\n",
      "False\n",
      "23\n",
      "False\n",
      "23\n",
      "False\n",
      "22\n",
      "False\n",
      "21\n",
      "False\n",
      "21\n",
      "False\n",
      "22\n",
      "False\n",
      "21\n",
      "False\n",
      "20\n",
      "False\n",
      "21\n",
      "False\n",
      "21\n",
      "False\n",
      "21\n",
      "False\n",
      "20\n",
      "False\n",
      "20\n",
      "False\n",
      "15\n",
      "False\n",
      "10\n",
      "False\n",
      "15\n",
      "False\n",
      "15\n",
      "False\n",
      "20\n",
      "False\n",
      "21\n",
      "False\n",
      "22\n",
      "False\n",
      "22\n",
      "False\n",
      "22\n",
      "False\n",
      "23\n",
      "False\n",
      "23\n",
      "False\n",
      "24\n",
      "False\n",
      "23\n",
      "False\n",
      "22\n",
      "False\n",
      "22\n",
      "False\n",
      "21\n",
      "False\n",
      "20\n",
      "False\n",
      "21\n",
      "False\n",
      "22\n",
      "False\n",
      "23\n",
      "False\n",
      "24\n",
      "False\n",
      "24\n",
      "False\n",
      "19\n",
      "False\n",
      "19\n",
      "False\n",
      "19\n",
      "False\n",
      "19\n",
      "False\n",
      "14\n",
      "False\n",
      "14\n",
      "False\n",
      "14\n",
      "False\n",
      "13\n",
      "False\n",
      "13\n",
      "False\n",
      "14\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "4\n",
      "False\n",
      "3\n",
      "False\n",
      "2\n",
      "False\n",
      "3\n",
      "False\n",
      "4\n",
      "False\n",
      "4\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "14\n",
      "False\n",
      "14\n",
      "False\n",
      "19\n",
      "False\n",
      "19\n",
      "False\n",
      "24\n",
      "False\n",
      "19\n",
      "False\n",
      "24\n",
      "False\n",
      "23\n",
      "False\n",
      "23\n",
      "False\n",
      "24\n",
      "False\n",
      "23\n",
      "False\n",
      "24\n",
      "False\n",
      "23\n",
      "False\n",
      "23\n",
      "False\n",
      "23\n",
      "False\n",
      "22\n",
      "False\n",
      "22\n",
      "False\n",
      "23\n",
      "False\n",
      "22\n",
      "False\n",
      "22\n",
      "False\n",
      "22\n",
      "False\n",
      "22\n",
      "False\n",
      "22\n",
      "False\n",
      "23\n",
      "False\n",
      "24\n",
      "False\n",
      "24\n",
      "False\n",
      "23\n",
      "False\n",
      "23\n",
      "False\n",
      "22\n",
      "False\n",
      "23\n",
      "False\n",
      "22\n",
      "False\n",
      "22\n",
      "False\n",
      "21\n",
      "False\n",
      "22\n",
      "False\n",
      "21\n",
      "False\n",
      "20\n",
      "False\n",
      "20\n",
      "False\n",
      "15\n",
      "False\n",
      "15\n",
      "False\n",
      "20\n",
      "False\n",
      "21\n",
      "False\n",
      "21\n",
      "False\n",
      "20\n",
      "False\n",
      "20\n",
      "False\n",
      "20\n",
      "False\n",
      "20\n",
      "False\n",
      "21\n",
      "False\n",
      "22\n",
      "False\n",
      "22\n",
      "False\n",
      "22\n",
      "False\n",
      "21\n",
      "False\n",
      "22\n",
      "False\n",
      "23\n",
      "False\n",
      "23\n",
      "False\n",
      "23\n",
      "False\n",
      "23\n",
      "False\n",
      "22\n",
      "False\n",
      "22\n",
      "False\n",
      "23\n",
      "False\n",
      "24\n",
      "False\n",
      "24\n",
      "False\n",
      "24\n",
      "False\n",
      "24\n",
      "False\n",
      "23\n",
      "False\n",
      "23\n",
      "False\n",
      "23\n",
      "False\n",
      "23\n",
      "False\n",
      "24\n",
      "False\n",
      "19\n",
      "False\n",
      "14\n",
      "False\n",
      "14\n",
      "False\n",
      "19\n",
      "False\n",
      "14\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "4\n",
      "False\n",
      "4\n",
      "False\n",
      "4\n",
      "False\n",
      "3\n",
      "False\n",
      "4\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "14\n",
      "False\n",
      "9\n",
      "False\n",
      "4\n",
      "False\n",
      "4\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "4\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "4\n",
      "False\n",
      "9\n",
      "False\n",
      "4\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "4\n",
      "False\n",
      "4\n",
      "False\n",
      "4\n",
      "False\n",
      "9\n",
      "False\n",
      "14\n",
      "False\n",
      "19\n",
      "False\n",
      "19\n",
      "False\n",
      "24\n",
      "False\n",
      "24\n",
      "False\n",
      "24\n",
      "False\n",
      "19\n",
      "False\n",
      "19\n",
      "False\n",
      "19\n",
      "False\n",
      "19\n",
      "False\n",
      "24\n",
      "False\n",
      "19\n",
      "False\n",
      "19\n",
      "False\n",
      "14\n",
      "False\n",
      "9\n",
      "False\n",
      "9\n",
      "False\n",
      "4\n",
      "False\n",
      "3\n",
      "False\n",
      "3\n",
      "False\n",
      "2\n",
      "False\n",
      "3\n",
      "False\n",
      "2\n",
      "False\n",
      "2\n",
      "False\n",
      "2\n",
      "False\n",
      "1\n",
      "False\n",
      "0\n",
      "False\n",
      "1\n",
      "False\n",
      "0\n",
      "False\n",
      "0\n",
      "False\n",
      "1\n",
      "False\n",
      "0\n",
      "False\n",
      "5\n",
      "False\n",
      "10\n",
      "False\n",
      "10\n",
      "False\n",
      "11\n",
      "False\n",
      "12\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "nr.seed(4567)    \n",
    "\n",
    "for i in MC_generate_episode(policy, neighbors, 12):\n",
    "    print(i)\n",
    "    print(i in taboo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine your results and answer the following questions to ensure your code has operated correctly:\n",
    "1. Have any taboo states been visited? ANS: No. \n",
    "2. Does the random walk end at the terminal state? ANS: Yes, at 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to create a function to compute the first visit action values along the random walk path. You are welcome to start with the `MC_state_values` function from the MC RL notebook. Notice that this function is incorrectly named, as it actually does compute action values.  \n",
    "\n",
    "Execute your function for 1,000 episodes. Make sure your code has 0 action values for the taboo states and the goal (terminal) state. If not, something is likely wrong with your random walk generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def MC_state_values(policy, neighbors, rewards, terminal, episodes = 1):\n",
    "    '''Function for first visit Monte Carlo on GridWorld.'''\n",
    "    ## Create list of states \n",
    "    states = list(policy.keys())\n",
    "    n_states = len(states)\n",
    "    \n",
    "    ## An array to hold the accumulated returns as we visit states\n",
    "    G = np.zeros((episodes,n_states))\n",
    "    \n",
    "    ## An array to keep track of how many times we visit each state so we can \n",
    "    ## compute the mean\n",
    "    n_visits = np.zeros((n_states))\n",
    "    \n",
    "    ## Iterate over the episodes\n",
    "    for i in range(episodes):\n",
    "        ## For each episode we use a list to keep track of states we have visited.\n",
    "        ## Once we visit a state we need to accumulate values to get the returns\n",
    "        states_visited = []\n",
    "   \n",
    "        ## Get a path for this episode\n",
    "        visit_list = MC_generate_episode(policy, neighbors, terminal)\n",
    "        current_state = visit_list[0]\n",
    "        for state in visit_list[0:]: \n",
    "            ## list of states we can transition to from current state\n",
    "            transition_list = list(neighbors[current_state].values())\n",
    "            \n",
    "            if(state in transition_list): # Make sure the transistion is allowed\n",
    "                transition_index = transition_list.index(state)   \n",
    "  \n",
    "                ## find the action value for the state transition\n",
    "                v_s = list(rewards[current_state].values())[transition_index]\n",
    "   \n",
    "                ## Mark that the current state has been visited \n",
    "                if(state not in states_visited): states_visited.append(current_state)  \n",
    "                ## Loop over the states already visited to add the value to the return\n",
    "                for visited in states_visited:\n",
    "                    G[i,visited] = G[i,visited] + v_s\n",
    "                    n_visits[visited] = n_visits[visited] + 1.0\n",
    "            ## Update the current state for next transition\n",
    "            current_state = state   \n",
    "    \n",
    "    ## Compute the average of G over the episodes are return\n",
    "    n_visits = [nv if nv != 0.0 else 1.0 for nv in n_visits]\n",
    "    returns = np.divide(np.sum(G, axis = 0), n_visits)   \n",
    "    return(returns)              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.43018543, -0.43923329, -0.44029618, -0.45050552, -0.43947299],\n",
       "       [-0.4109179 ,  0.        ,  0.        ,  0.        , -0.41474819],\n",
       "       [-0.35659968,  0.03120994,  0.        ,  0.09269507, -0.36515248],\n",
       "       [-0.3954133 ,  0.        ,  0.        ,  0.        , -0.403777  ],\n",
       "       [-0.4127993 , -0.42663166, -0.43609374, -0.43173438, -0.42296307]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr.seed(4567)\n",
    "returns = MC_state_values(policy, neighbors, reward, terminal = 12, episodes = 1000)\n",
    "np.array(returns).reshape((5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine your results and answer the following questions to ensure you action value function operates correctly:\n",
    "1. Are the values of the taboo states 0? ANS: Yes, you can see above.\n",
    "2. Are the states with the highest values adjacent to the terminal state? ANS: Yes, 11, 13 values are highest, not same though.\n",
    "3. Are the values of the states decreasing as the distance from the terminal state increases? ANS: Yes, but not symmetric. Weirdly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Policy Improvement\n",
    "\n",
    "Now that you have your representation and a functions to perform the MC policy evaluation you have everything you need to apply the policy improvement algorithm to create an optimal policy for the robot to reach the goal. \n",
    "\n",
    "If your policy evaluation functions work correctly, you should be able to use the `MC_optimal_policy` function from the MC RL notebook. Execute your code using 5 cycles of 1000 episodes for policy improvement. Set $\\epsilon = 0.01$. Computation may take some time. Make sure you print the state values as well as the policy you discovered. \n",
    "\n",
    "> **Note:** There is some chance the algorithm will not converge from certain starting states. This seems to result from accumulating small errors over the many iterations. The solution should be to change seeds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def MC_optimal_policy(policy, neighbors, rewards, terminal, episodes = 10, cycles = 10, epsilon = 0.05):\n",
    "    ## Create a working cooy of the initial policy\n",
    "    current_policy = copy.deepcopy(policy)\n",
    "    \n",
    "    ## Loop over a number of cycles of GPI\n",
    "    for _ in range(cycles):\n",
    "        ## First compute the average returns for each of the states. \n",
    "        ## This is the policy evaluation phase\n",
    "        returns = MC_state_values(current_policy, neighbors, rewards, terminal = terminal, episodes = episodes)\n",
    "        \n",
    "        ## We want max Q for each state, where Q is just the difference \n",
    "        ## in the values of the possible state transition\n",
    "        ## This is the policy evaluation phase\n",
    "        for s in current_policy.keys(): # iterate over all states\n",
    "            ## Compute Q for each possible state transistion\n",
    "            ## Start by creating a list of the adjacent states.\n",
    "            possible_s_prime = neighbors[s]\n",
    "            neighbor_states = list(possible_s_prime.values())\n",
    "            ## Check if terminal state is neighbor, but state is not terminal.\n",
    "            if(terminal in neighbor_states and s != terminal):\n",
    "                ## account for the special case adjacent to goal\n",
    "                neighbor_Q = []\n",
    "                for s_prime in possible_s_prime.keys(): # Iterate over adjacent states\n",
    "                    if(neighbors[s][s_prime] == terminal):  \n",
    "                         neighbor_Q.append(returns[s])\n",
    "                    else: neighbor_Q.append(0.0) ## Other transisions have 0 value.   \n",
    "            else: \n",
    "                 ## The other case is rather easy. Compute Q for the transistion to each neighbor           \n",
    "                 neighbor_values = returns[neighbor_states]\n",
    "                 neighbor_Q = [n_val - returns[s] for n_val in neighbor_values]\n",
    "                \n",
    "            ## Find the index for the state transistions with the largest values \n",
    "            ## May be more than one. \n",
    "            max_index = np.where(np.array(neighbor_Q) == max(neighbor_Q))[0]  \n",
    "            \n",
    "            ## Probabilities of transition\n",
    "            ## Need to allow for further exploration so don't let any \n",
    "            ## transition probability be 0.\n",
    "            ## Some gymnastics are required to ensure that the probabilities \n",
    "            ## over the transistions actual add to exactly 1.0\n",
    "            neighbors_len = float(len(np.array(neighbor_Q)))\n",
    "            max_len = float(len(max_index))\n",
    "            diff = round(neighbors_len - max_len,3)\n",
    "            prob_for_policy = round(1.0/max_len,3)\n",
    "            adjust = round((epsilon * (diff)), 3)\n",
    "            prob_for_policy = prob_for_policy - adjust\n",
    "            if(diff != 0.0):\n",
    "                remainder = (1.0 - max_len * prob_for_policy)/diff\n",
    "            else:\n",
    "                remainder = epsilon\n",
    "                                                 \n",
    "            for i, key in enumerate(current_policy[s]): ## Update policy\n",
    "                if(i in max_index): current_policy[s][key] = prob_for_policy\n",
    "                else: current_policy[s][key] = remainder          \n",
    "                   \n",
    "    return current_policy\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'d': 0.97,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 1: {'d': 0.010000000000000009,\n",
       "  'l': 0.97,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 2: {'d': 0.010000000000000009,\n",
       "  'l': 0.97,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 3: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.97,\n",
       "  'u': 0.010000000000000009},\n",
       " 4: {'d': 0.97,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 5: {'d': 0.97,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 6: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 7: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 8: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 9: {'d': 0.97,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 10: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.97,\n",
       "  'u': 0.010000000000000009},\n",
       " 11: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.97,\n",
       "  'u': 0.010000000000000009},\n",
       " 12: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 13: {'d': 0.010000000000000009,\n",
       "  'l': 0.97,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 14: {'d': 0.010000000000000009,\n",
       "  'l': 0.97,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 15: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.97},\n",
       " 16: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 17: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 18: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 19: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.97},\n",
       " 20: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.97},\n",
       " 21: {'d': 0.010000000000000009,\n",
       "  'l': 0.97,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 22: {'d': 0.010000000000000009,\n",
       "  'l': 0.97,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 23: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.97,\n",
       "  'u': 0.010000000000000009},\n",
       " 24: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.97}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr.seed(4567)    \n",
    "MC_policy = MC_optimal_policy(policy, neighbors, reward, terminal = 12, episodes = 1000, cycles = 5, \n",
    "                              epsilon = 0.01)  \n",
    "MC_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine your results and answer the following questions:\n",
    "1. Based on the action values, does it appear that the algorithm is near convergence with only small changes? ANS: Yes, pretty much.\n",
    "2. Assume the robot enters the warehouse though a door at state (position) 2. As you did for the DP exercise, create an in illustration of the optimal path to the goal based on the policy your algorithm discovered. Replace the figure below with your illustration.\n",
    "\n",
    "<img src=\"HW7.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **Replace this diagram with one showing your solution** </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, test your improved policy by computing the action values. In the cell below, create and execute the code to display the action values for all the states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0808656 , 1.56596273, 1.38461538, 1.43333333, 1.92451155],\n",
       "       [2.8382104 , 0.        , 0.        , 0.        , 2.70530973],\n",
       "       [4.23679031, 8.94736842, 0.        , 8.66159696, 4.15177903],\n",
       "       [2.83665768, 0.        , 0.        , 0.        , 2.76753022],\n",
       "       [2.12977444, 1.60025445, 1.38461538, 1.65454545, 2.01470588]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr.seed(4567)\n",
    "returns2 = MC_state_values(MC_policy, neighbors, reward, terminal = 12, episodes = 1000)\n",
    "np.array(returns2).reshape((5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine your results and answer the following questions. \n",
    "\n",
    "1. Do your action values represent a significant improvement over the initial random policy? ANS: \n",
    "\n",
    "Quiet a lots. Every value becomes a positive, and the highest one is 8.94 from .0092\n",
    "\n",
    "2. Do the states near the goal have action values near the terminal reward? ANS: Yes, they are 8.9 and 8.6, almost 10. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
